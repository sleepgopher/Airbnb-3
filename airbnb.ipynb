{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Analysis of the Airbnb data of London\n",
    "\n",
    "Airbnb is an online marketplace and hospitality service with millions of listings. In this project, we analyze the Airbnb data of London and try to use the available information to predict the review scores of each listing. The data sets are available [here](http://insideairbnb.com/get-the-data.html).\n",
    "\n",
    "##Reading data\n",
    "\n",
    "There are three files regarding the calendar, listings, and reviews. Among the three files, the listings data provide detailed information of the amenities, prices, description, etc. of all the listings. Thus, we focus on the listings data for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "listings = pd.read_csv('../Airbnb/src/data_sets/listings.csv.gz')\n",
    "listings = listings.rename(columns={'id': 'listing_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Clean and transform data\n",
    "\n",
    "As we want to predict the review scores, we remove all the missing values and bin it into five groups. Then we clean the data and transform the format of some of the variables that have (relatively) strong relationship with the review scores.\n",
    "\n",
    "Notice that special care has to be taken regarding the host verifications and amenities: Each item contains a list of features of each listing in a string format; thus, we first extract all the features in that string and then convert them into multiple variables of binary values where `1` indicates the presence of this feature and `0` indicates absence. Also, there are two amenities regarding missing translations; these are removed from the amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "resp = 'review_scores_rating'  # Set response\n",
    "listings = listings[~listings[resp].isnull()].reset_index(0, True)  # Remove missing values\n",
    "listings[resp] = pd.cut(listings[resp], np.arange(0, 120, 20), labels=np.arange(5),\n",
    "                        include_lowest=True)  # Bin response into five levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_zipcode(df, variable='zipcode'):\n",
    "    \"\"\"Transform zipcode to outward code\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :param variable: zipcode\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[variable] = df[variable].fillna('').str.lower().str.replace(r' +', '') \\\n",
    "        # Reformat zipcode\n",
    "    pat = df[variable].str.match(r'^[a-z]{1,2}[0-9][a-z0-9]?([0-9][a-z]{2})?$') \\\n",
    "        # Find zipcode of correct format\n",
    "    df.loc[~pat, variable] = ''  # Remove wrongly recorded zipcode\n",
    "    df[variable] = df[variable].apply(lambda x: x if len(x) <= 4 else x[:-3]) \\\n",
    "        # Extract outward code (this step is redundant)\n",
    "    df[variable] = df[variable].str.extract(r'([a-z]{1,2})', expand=False) \\\n",
    "        # Extract postcode area\n",
    "    return df\n",
    "\n",
    "\n",
    "listings = transform_zipcode(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "def transform_label(df, variables=None):\n",
    "    \"\"\"Binarize labels using numerical values\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :param variables: list of variables to be binarized\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    transformed_list = []\n",
    "    \n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].fillna('')  # Reformat variables\n",
    "        transformed = label_binarizer.fit_transform(df[variable])  # Binarize variables\n",
    "        \n",
    "        # Set column names of the transformed data\n",
    "        if transformed.shape[1] == 1:\n",
    "            transformed_columns = [variable]\n",
    "        else:\n",
    "            columns = pd.Series(label_binarizer.classes_).str.lower() \\\n",
    "                .str.replace(r'[&\\-/ ]', '_').str.replace(r'_+', '_')\n",
    "            transformed_columns = [variable + '.' + column for column in columns]\n",
    "        \n",
    "        transformed = pd.DataFrame(transformed, columns=transformed_columns)\n",
    "        transformed_list.append(transformed)  # Add transformed data to the transformed list\n",
    "        df = df.drop(variable, 1)  # Remove original data\n",
    "        \n",
    "    df = pd.DataFrame(pd.concat([df] + transformed_list, 1))  # Concatenate transformed data\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_label = [\n",
    "    'experiences_offered',\n",
    "    'host_response_time',\n",
    "    'zipcode',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'bed_type',\n",
    "    'cancellation_policy',\n",
    "    'host_is_superhost',\n",
    "    'is_location_exact',\n",
    "    'requires_license',\n",
    "    'instant_bookable',\n",
    "    'require_guest_profile_picture',\n",
    "    'require_guest_phone_verification'\n",
    "]\n",
    "listings = transform_label(listings, vars_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_host_since(df):\n",
    "    \"\"\"Transform host start date to duration\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'], yearfirst=True) \\\n",
    "        # Convert to datetime format\n",
    "    df['host_for'] = (df['host_since'].max() - df['host_since']).dt.days \\\n",
    "        # Calculate the duration of hosting\n",
    "    return df\n",
    "\n",
    "\n",
    "listings = transform_host_since(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def transform_text(df, variables=None, max_features=100):\n",
    "    \"\"\"Transform variables consisting of text into a sparse format\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :param variables: list of variables to be transformed\n",
    "    :param max_features: maximum number of features\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    count_vectorizer = CountVectorizer(stop_words='english', max_features=max_features)\n",
    "    transformed_list = []\n",
    "    \n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].fillna('')\n",
    "        transformed = count_vectorizer.fit_transform(df[variable])\n",
    "        \n",
    "        # Set column names of the transformed data\n",
    "        columns = sorted(count_vectorizer.vocabulary_.keys())\n",
    "        transformed_columns = [variable + '.' + column for column in columns]\n",
    "        \n",
    "        transformed = pd.DataFrame(transformed.toarray(), columns=transformed_columns)\n",
    "        transformed_list.append(transformed)  # Add transformed data to the transformed list\n",
    "        df = df.drop(variable, 1)  # Remove original data\n",
    "        \n",
    "    df = pd.DataFrame(pd.concat([df] + transformed_list, 1))  # Concatenate transformed data\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_text = ['host_verifications']\n",
    "listings = transform_text(listings, vars_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_percent(df, variables=None):\n",
    "    \"\"\"Transform strings of percentages to decimals\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :param variables: list of variables to be transformed\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].str.strip('%')\n",
    "        df[variable] = df[variable].astype(np.float64) / 100\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_percent = ['host_response_rate']\n",
    "listings = transform_percent(listings, vars_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_price(df, variables=None):\n",
    "    \"\"\"Transform strings of price to numerals\n",
    "    \n",
    "    :param df: data framed to be transformed\n",
    "    :param variables: list of variables to be transformed\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].str.strip('$').str.replace(',', '') \\\n",
    "            # Remove dollar signs and thousands separators\n",
    "        df[variable] = df[variable].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_price = ['price']\n",
    "listings = transform_price(listings, vars_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_amenities(df, variable='amenities'):\n",
    "    \"\"\"Extract amenities\n",
    "    \n",
    "    :param df: data frame to be transformed\n",
    "    :param variable: amenities\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[variable] = df[variable].str.replace(r'[:\\-\\./ ]', '_').str.replace(r'[\\(\\)]', '') \\\n",
    "        .str.replace(r'_+', '_').str.lower()  # Reformat amenities\n",
    "    df = transform_text(df, [variable])  # Transform amenities using transform_text\n",
    "    columns_to_remove = [column for column in df.columns if 'missing' in column]\n",
    "    df = pd.DataFrame(df.drop(columns_to_remove, 1))  # Remove unwanted amenities\n",
    "    return df\n",
    "\n",
    "\n",
    "listings = transform_amenities(listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the format of the variables, we remove those having high percentage of missing values and/or having little or no relationship with the review scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = listings.drop([\n",
    "    'listing_id',\n",
    "    'listing_url',\n",
    "    'scrape_id',\n",
    "    'last_scraped',\n",
    "    'name',\n",
    "    'summary',\n",
    "    'space',\n",
    "    'description',\n",
    "    'neighborhood_overview',\n",
    "    'notes',\n",
    "    'transit',\n",
    "    'access',\n",
    "    'interaction',\n",
    "    'house_rules',\n",
    "    'thumbnail_url',\n",
    "    'medium_url',\n",
    "    'picture_url',\n",
    "    'xl_picture_url',\n",
    "    'host_url',\n",
    "    'host_name',\n",
    "    'host_since',\n",
    "    'host_location',\n",
    "    'host_about',\n",
    "    'host_acceptance_rate',\n",
    "    'host_thumbnail_url',\n",
    "    'host_picture_url',\n",
    "    'host_neighbourhood',\n",
    "    'host_listings_count',\n",
    "    'host_total_listings_count',\n",
    "    'host_has_profile_pic',\n",
    "    'host_identity_verified',\n",
    "    'street',\n",
    "    'neighbourhood',\n",
    "    'neighbourhood_cleansed',\n",
    "    'neighbourhood_group_cleansed',\n",
    "    'city',\n",
    "    'state',\n",
    "    'market',\n",
    "    'smart_location',\n",
    "    'country_code',\n",
    "    'country',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'is_location_exact',\n",
    "    'square_feet',\n",
    "    'weekly_price',\n",
    "    'monthly_price',\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "    'guests_included',\n",
    "    'extra_people',\n",
    "    'minimum_nights',\n",
    "    'maximum_nights',\n",
    "    'calendar_updated',\n",
    "    'has_availability',\n",
    "    'availability_30',\n",
    "    'availability_60',\n",
    "    'availability_90',\n",
    "    'availability_365',\n",
    "    'calendar_last_scraped',\n",
    "    'first_review',\n",
    "    'last_review',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'requires_license',\n",
    "    'license',\n",
    "    'jurisdiction_names',\n",
    "    'instant_bookable',\n",
    "    'require_guest_profile_picture',\n",
    "    'require_guest_phone_verification',\n",
    "    'reviews_per_month'\n",
    "], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are too many variables regarding host verifications and amenities, and some of them may be redundant to predict the review scores. For example, almost every host is verified by phone, so this variable may be of little significance. Therefore, we perform $\\chi^{2}$ tests to contract the feature space by selecting those with a significantly small $p$-value. However, the analysis of significance should be performed using the train data only; thus, we impute the data and split it into the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "imputer = Imputer(strategy='median')  # Replace missing values with the median\n",
    "listings_imputed = imputer.fit_transform(listings)\n",
    "\n",
    "train, test = train_test_split(listings_imputed)  # Split the data into train and test sets\n",
    "train = pd.DataFrame(train, columns=listings.columns)\n",
    "test = pd.DataFrame(test, columns=listings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "def remove_redundant_features(train, test, variables=None, resp='review_scores_rating'):\n",
    "    \"\"\"Perform chi-squared test to remove sparse features of low significance\n",
    "    \n",
    "    :param train: train data frame to be transformed\n",
    "    :param test: test data frame to be transformed\n",
    "    :param variables: list of ``sparse`` variables to be transformed\n",
    "    :param resp: response\n",
    "    :return: transformed data frame\n",
    "    \"\"\"\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    for variable in variables:\n",
    "        variables_list = [item for item in train.columns if variable + '.' in item] \\\n",
    "            # Find sparse features\n",
    "        tmp = train[variables_list + [resp]]  # Form data frame of sparse feature and response\n",
    "        _, p_val = chi2(tmp[variables_list], tmp[resp])  # Perform chi-squared test\n",
    "        variables_list = list(compress(variables_list, (p_val > 0.05))) \\\n",
    "            # Find sparse features of low significance\n",
    "        train = train.drop(variables_list, 1)  # Remove sparse features of low significance\n",
    "        test = test.drop(variables_list, 1)  # Remove sparse features of low significance\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = remove_redundant_features(train, test, ['host_verifications', 'amenities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Predict the review scores\n",
    "\n",
    "After removing the redundant variables, we train a random forest classifier to predict the review scores. However, since the review scores are imbalanced (reviewers tend to give high scores), we upsample the classes of small sample size before training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "train_resampled_list = []\n",
    "max_samples = train[resp].value_counts().max() \\\n",
    "    # Find the class with maximum number of samples\n",
    "for i in range(5):\n",
    "    resampled = resample(train[train[resp] == i], n_samples=max_samples)  # Upsample\n",
    "    train_resampled_list.append(resampled)  # Add upsampled data to the transformed list\n",
    "\n",
    "train_resampled = pd.DataFrame(pd.concat(train_resampled_list, ignore_index=True)) \\\n",
    "    # Concatenate upsampled data\n",
    "\n",
    "X_train = train_resampled.drop([resp], 1)\n",
    "y_train = train_resampled[resp]\n",
    "X_test = test.drop([resp], 1)\n",
    "y_test = test[resp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3014864494413203"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `GridSearchCV` to tune the parameters of the estimator based on the train set. Notice that we use the logarithmic loss as the evaluation metric; thus, we need to redefine the scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=250, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [250, 500],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "scoring = make_scorer(log_loss, False, True)\n",
    "\n",
    "clf = GridSearchCV(estimator, param_grid, scoring, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the best estimator to predict the review scores of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54126133477354821"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.best_estimator_.predict_proba(X_test)\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further enhance the prediction accuracy, the reviews data can be incorporated as well. It may contain useful information such as the number of reviews made by return customers and the keywords leading to high review scores. This will be considered in the second part of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}